# app/github_crawling/scheduler.py

from app.github_crawling.github_team_repos_from_urls import get_all_repos_from_team_urls
from app.github_crawling.github_api import fetch_commits, fetch_prs, fetch_readme, fetch_closed_issues
from app.github_crawling.vector_store import store_document, is_id_exists, show_vector_summary
from collections import defaultdict
from app.github_crawling.github_api import fetch_wiki_md_files
from app.model_inference.rag_chat_runner import embedding_model
from dotenv import load_dotenv
load_dotenv()

import json
from datetime import date, timedelta
from pathlib import Path
from datetime import datetime
from dateutil.parser import parse
from app.github_crawling.vector_store import delete_document_if_exists
from app.model_inference.loaders.gemini_langchain_llm import summarize_chain

PART_LIST = ["ai", "be", "cloud", "fe", "wiki"]
def classify_part_from_repo(repo_name: str) -> str:
    lowered = repo_name.lower()
    for part in PART_LIST:
        if f"-{part}" in lowered:
            return part.upper()
    return "others"

LAST_RUN_FILE = Path("last_run_date.json")
FORCE_RUN = False # ì¼ì£¼ì¼ ë‹¨ìœ„ê°€ ì•„ë‹Œ ë°”ë¡œ ì‹¤í–‰

def is_weekly_run_due() -> bool:
    if not LAST_RUN_FILE.exists():
        return True
    with open(LAST_RUN_FILE, "r") as f:
        last_run_str = json.load(f).get("last_run", "")
        try:
            last_run = date.fromisoformat(last_run_str)
            return (date.today() - last_run) >= timedelta(days=7)
        except:
            return True

def update_last_run_date():
    with open(LAST_RUN_FILE, "w") as f:
        json.dump({"last_run": date.today().isoformat()}, f)

def clean_iso_date(iso_str: str) -> str:
    return iso_str.replace("Z", "+00:00")

def generate_week_ranges(start_date: datetime, end_date: datetime):
    week_ranges = []
    start = start_date
    end = end_date
    while start < end:
        week_start = start
        week_end = min(start + timedelta(days=6), end)
        week_ranges.append((week_start.date().isoformat(), week_end.date().isoformat()))
        start += timedelta(days=7)
    return week_ranges


def group_data_by_week(data, week_ranges):
    weekly_data = defaultdict(list)
    for item in data:
        item_date = datetime.fromisoformat(clean_iso_date(item["date"])).date()
        for week_start, week_end in week_ranges:
            if week_start <= item_date.isoformat() <= week_end:
                weekly_data[(week_start, week_end)].append(item)
                break
    return weekly_data


def summarize_weekly_data(weekly_data_dict, repo, project_id, team_id):
    part = classify_part_from_repo(repo)

    for (week_start, week_end), items in weekly_data_dict.items():
        if not items:
            continue

        doc_id = f"summary-{team_id}-{part}-{week_end[5:7]}-{week_end[8:10]}"
        if is_id_exists(doc_id):
            print(f"âœ… ì´ë¯¸ ì €ì¥ëœ ë¬¸ì„œ: {doc_id} â†’ ìƒëµ")
            continue

        #ìš”ì•½ ì§ì ‘ ì‹¤í–‰
        raw_text = "\n".join(item.get("message", item.get("title", "")) for item in items)
        print(f"ğŸ” Gemini ìš”ì•½ ì¤‘... Team: {team_id}, Part: {part}")
        summary = summarize_chain.invoke({"input": raw_text})

        #ë©”íƒ€ë°ì´í„° êµ¬ì„±
        metadata = {
            "repo": repo,
            "date": week_end,
            "project_id": project_id,
            "team_id": team_id,
            "type": "summary",
            "part": part
        }

        #ì§ì ‘ store_document í˜¸ì¶œ (ê°€ì¤‘ì¹˜ ìë™ ì ìš©ë¨)
        print(f"ğŸ“… ìš”ì•½ ê²°ê³¼ ì €ì¥ ì¤‘... ID: {doc_id}")
        store_document(summary, metadata, embedding_model, doc_id)

def summarize_wiki_pages(repo, project_id, team_id):
    pages = fetch_wiki_md_files(repo)
    if not pages:
        print("âŒ Wiki pagesê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    part = "WIKI"
    wiki_date = list(pages.values())[0]["date"]  # ìµœì‹  ì»¤ë°‹ì¼

    sorted_page_items = sorted(pages.items())  # í˜ì´ì§€ ì´ë¦„ ê¸°ì¤€ ì •ë ¬
    chunk_size = 10
    chunk_count = (len(sorted_page_items) + chunk_size - 1) // chunk_size

    for i in range(chunk_count):
        chunk = sorted_page_items[i * chunk_size: (i + 1) * chunk_size]

        combined_text = ""
        for fname, page in chunk:
            combined_text += f"# {fname}\n{page['content']}\n\n"

        chunk_id = i + 1
        doc_id = f"summary-{team_id}-{part}-{wiki_date[5:7]}-{wiki_date[8:10]}-chunk{chunk_id:02}"

        # ì´ë¯¸ ìˆë‹¤ë©´ ì‚­ì œ í›„ ë‹¤ì‹œ ìƒì„±
        if is_id_exists(doc_id):
            print(f"ğŸ—‘ï¸ ê¸°ì¡´ ìš”ì•½ ë¬¸ì„œ ì‚­ì œ: {doc_id}")
            delete_document_if_exists(doc_id)

        metadata = {
            "repo": repo,
            "date": wiki_date,
            "project_id": project_id,
            "team_id": team_id,
            "type": "summary",
            "part": part,
            "chunk": chunk_id
        }

        print(f"ğŸ§¾ wiki ë¬¸ì„œ ìš”ì•½ ì €ì¥: {doc_id} (í˜ì´ì§€ {i * chunk_size + 1}~{(i + 1) * chunk_size})")
        summary = summarize_chain.invoke({"input": combined_text.strip()})
        store_document(summary, metadata, embedding_model, doc_id)

def main():
    should_run = FORCE_RUN or is_weekly_run_due()

    if not should_run:
        has_existing_data = LAST_RUN_FILE.exists()
        if has_existing_data:
            print("â¡ï¸ ì´ë²ˆ ì£¼ëŠ” ì´ë¯¸ ì‹¤í–‰ë¨. ìƒëµí•©ë‹ˆë‹¤.")
            return

    # run ì¡°ê±´ì„ í†µê³¼í•œ ê²½ìš°ì—ë§Œ ì‹¤í–‰
    update_last_run_date()
    print("ğŸš€ ì¼ì£¼ì¼ì— í•œ ë²ˆ ì‹¤í–‰ë˜ëŠ” ìš”ì•½ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    
    REPOS = get_all_repos_from_team_urls()

    for repo_entry in REPOS:
        repo, project_id, team_id = repo_entry
        print(f"\nğŸš€ Start crawling: {repo} (Team ID: {project_id}, Number: {team_id})")

        if repo.endswith("-wiki"): # wikiëŠ” ë³„ë„ ì²˜ë¦¬
            summarize_wiki_pages(repo, project_id, team_id)
            continue  # ì•„ë˜ í¬ë¡¤ë§ ë¡œì§ ê±´ë„ˆëœ€

        all_items = []
        readme_data = fetch_readme(repo)
        if readme_data:
            all_items.append({
                "type": "readme",
                "repo": repo,
                "title": "README",
                "body": readme_data["content"],
                "date": readme_data["date"]
            })

        for item in fetch_commits(repo):
            all_items.append(item)
        for item in fetch_prs(repo):
            all_items.append(item)
        for issue in fetch_closed_issues(repo):
            all_items.append({
                "type": "issue",
                "repo": repo,
                "title": issue["title"],
                "body": issue["body"],
                "date": issue["closed_at"]
            })

        print(f"ğŸ“… í¬ë¡¤ë§ëœ ì „ì²´ í•­ëª© ìˆ˜: {len(all_items)}")
        for item in all_items:
            print(" -", item.get("date", "ë‚ ì§œ ì—†ìŒ"), item.get("type", "unknown"))

        if not all_items:
            continue

        # ë‚ ì§œ íŒŒì‹± ë° ìœ íš¨ í•­ëª© í•„í„°ë§
        valid_dates = []
        for item in all_items:
            date_str = item.get("date")
            try:
                parsed_date = parse(date_str)
                valid_dates.append(parsed_date)
                item["parsed_date"] = parsed_date  # ë‚˜ì¤‘ì— ê·¸ë£¹í•‘ì— í™œìš© ê°€ëŠ¥
            except Exception:
                print("âš ï¸ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨:", date_str)

        if not valid_dates:
            print("âŒ ë‚ ì§œ ì •ë³´ê°€ ì—†ì–´ ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            continue

        date_min = min(valid_dates)
        date_max = max(valid_dates)

        print("ğŸ“† date_min:", date_min)
        print("ğŸ“† date_max:", date_max)

        week_ranges = generate_week_ranges(date_min, date_max)
        print("ğŸ“… ìƒì„±ëœ ì£¼ì°¨ ë¦¬ìŠ¤íŠ¸:")
        for start, end in week_ranges:
            print(f" - {start} ~ {end}")

        grouped = group_data_by_week(all_items, week_ranges)
        for week, items in grouped.items():
            print(f"ğŸ—“ï¸ {week[0]} ~ {week[1]}: {len(items)}ê°œ í•­ëª©")

        summarize_weekly_data(grouped, repo, project_id, team_id)

if __name__ == "__main__":
    main()
    show_vector_summary()

# PYTHONPATH=. python app/github_crawling/scheduler.py
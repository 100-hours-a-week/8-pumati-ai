# app/github_crawling/scheduler.py

from github_team_repos import get_all_repos_from_team_urls
from github_api import fetch_commits, fetch_prs, fetch_readme
from parser import parse_commit_item, parse_pr_item, parse_readme
from text_splitter import split_text
from embedding import get_embedding
from vector_store import store_document

REPOS = get_all_repos_from_team_urls()

# def main():
#     for repo in REPOS:
#         print(f"\nğŸš€ Start crawling: {repo}")
        
#         commits = fetch_commits(repo)
#         print(f"ğŸ“ ì»¤ë°‹ ìˆ˜: {len(commits)}")
        
#         prs = fetch_prs(repo)
#         print(f"ğŸ” PR ìˆ˜: {len(prs)}")
        
#         readme = fetch_readme(repo)
#         print(f"ğŸ“˜ README ìˆìŒ?: {'ìˆìŒ' if readme else 'ì—†ìŒ'}")

def main():
    for repo in REPOS:
        # ì»¤ë°‹
        for item in fetch_commits(repo):
            parsed = parse_commit_item(item)
            print(f"[ğŸ“„ ì»¤ë°‹ í¬ë¡¤ë§ ê²°ê³¼] repo={repo} / date={item['date']} / len={len(parsed)}")
            for chunk in split_text(parsed):
                store_document(chunk, {"repo": repo, "date": item["date"]}, get_embedding(chunk))
        
        # PR
        for item in fetch_prs(repo):
            parsed = parse_pr_item(item)
            print(f"[ğŸ“„ PR í¬ë¡¤ë§ ê²°ê³¼] repo={repo} / date={item['date']} / len={len(parsed)}")
            for chunk in split_text(parsed):
                store_document(chunk, {"repo": repo, "date": item["date"]}, get_embedding(chunk))

        # README
        content = fetch_readme(repo)
        if content:
            parsed = parse_readme(content)
            print(f"[ğŸ“„ README í¬ë¡¤ë§ ê²°ê³¼] repo={repo} / len={len(parsed)}")
            for chunk in split_text(parsed):
                store_document(chunk, {"repo": repo, "date": "README"}, get_embedding(chunk))

if __name__ == "__main__":
    main()

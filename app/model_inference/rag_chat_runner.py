# app/model_inference/rag_chat_runner.py

from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from app.model_inference.loaders.hyperclova_langchain_llm import HyperClovaLangChainLLM

# 1. ì„ë² ë”© ëª¨ë¸ ì„¸íŒ… (E5-Base)
embedding_model = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-base")

# 2. Chroma ë¡œë“œ (DB ìœ„ì¹˜ ë° ì»¬ë ‰ì…˜ ì´ë¦„ ì§€ì •)
vectorstore = Chroma(
    collection_name="github_docs",
    persist_directory="./chroma_db_no_llm_e5_base",
    embedding_function=embedding_model
)

# 3. í”„ë¡œì íŠ¸ë³„ í•„í„°ë§ retriever ì„¤ì • (kê°’ ì¦ê°€)
def get_retriever(project_id: int):
    return vectorstore.as_retriever(search_kwargs={
        "k": 30,
        "filter": {"project_id": project_id}
    })

# 4. ì§ˆë¬¸ ì¬ì‘ì„± í•¨ìˆ˜ (ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜)
def rewrite_question(original: str) -> str:
    if "AI" in original:
        return "ìš´ì„¸ ìƒì„±, AI ëª¨ë¸ ì—°ë™, API ê°œë°œê³¼ ê´€ë ¨ëœ ê¸°ëŠ¥ì„ ì•Œë ¤ì¤˜"
    return original

# 5. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
prompt_template = PromptTemplate(
    input_variables=["context", "question"],
    template="""
ì•„ë˜ëŠ” í•œ íŒ€ì˜ GitHub í™œë™ ë‚´ì—­ì…ë‹ˆë‹¤:

{context}

ì‚¬ìš©ì ì§ˆë¬¸: {question}

ì´ íŒ€ì˜ í™œë™ì„ ê¸°ë°˜ìœ¼ë¡œ í•µì‹¬ì ì¸ ë‹µë³€ì„ ìì—°ìŠ¤ëŸ½ê²Œ í•´ì¤˜.
í° ê¸°ëŠ¥ ë‹¨ìœ„ë¡œ ì •ë¦¬í•˜ê³ , ë„ˆë¬´ ê¸°ìˆ ì ì´ì§€ ì•Šê²Œ ì„¤ëª…í•´ì¤˜.
"""
)

# 6. LLM ì„¤ì • (HyperCLOVA)
llm = HyperClovaLangChainLLM()

# 7. RAG ì‹¤í–‰ í•¨ìˆ˜
def run_rag(question: str, project_id: int):
    retriever = get_retriever(project_id)
    rewritten_q = rewrite_question(question)

    # ë””ë²„ê¹…ìš© ì •ë³´ ì¶œë ¥
    print("\nğŸ§© ì¡´ì¬í•˜ëŠ” ì»¬ë ‰ì…˜ ëª©ë¡:", vectorstore._client.list_collections())
    print("ğŸ” ë©”íƒ€ë°ì´í„° ì˜ˆì‹œ:", vectorstore._collection.get(limit=3)["metadatas"])

    # ë¬¸ì„œ ë‚´ 'ìš´ì„¸' í¬í•¨ ì—¬ë¶€ í™•ì¸
    docs = vectorstore._collection.get(limit=100)["documents"]
    for i, doc in enumerate(docs):
        if "ìš´ì„¸" in doc:
            print(f"âœ… ìš´ì„¸ ê´€ë ¨ ë¬¸ì„œ ë°œê²¬! [Doc {i+1}]\n{doc[:300]}")

    # ê²€ìƒ‰ëœ ë¬¸ì„œ í™•ì¸
    results = retriever.get_relevant_documents(rewritten_q)
    print(f"\nğŸ” ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(results)}")
    for i, doc in enumerate(results):
        print(f"\nğŸ“„ Doc {i+1}")
        print("Content:", doc.page_content[:100])
        print("Metadata:", doc.metadata)

    # RetrievalQA ì²´ì¸ ì‹¤í–‰
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        chain_type_kwargs={"prompt": prompt_template}
    )


    return qa_chain.run(rewritten_q)

# ì˜ˆì‹œ ì‹¤í–‰
if __name__ == "__main__":
    q = "AIëŠ” ì–´ë–¤ ê¸°ëŠ¥ ê°œë°œí–ˆì–´?"
    answer = run_rag(q, project_id=1)
    print("\nğŸ§  Answer:\n", answer)



# PYTHONPATH=. python app/model_inference/rag_chat_runner.py

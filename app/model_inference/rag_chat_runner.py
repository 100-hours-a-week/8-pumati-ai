# app/model_inference/rag_chat_runner.py

import os
from typing import List
from dotenv import load_dotenv

from qdrant_client import QdrantClient
from qdrant_client.http.models import Filter, FieldCondition, MatchValue, SearchRequest

from langchain_core.runnables import RunnableConfig, RunnableLambda, Runnable
from langchain_core.retrievers import BaseRetriever
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.retrieval import create_retrieval_chain
from langchain.prompts import PromptTemplate
from langchain.schema import Document
from langchain_qdrant import QdrantVectorStore
from langchain_huggingface import HuggingFaceEmbeddings

from langsmith import traceable, client

from app.context_construction.question_router import is_structured_question, classify_question_type
from app.context_construction.prompts.chat_prompt import build_prompt_template, general_prompt_template
from app.model_inference.loaders.gemini import GeminiLangChainLLM

os.environ["TOKENIZERS_PARALLELISM"] = "false"

FILTERED_RESPONSE = """\
ğŸ’­ ì €ëŠ” íŒ€ í”„ë¡œì íŠ¸ ì „ìš© AI, í’ˆì•—ì´(pumati)ì˜ ë§ˆí‹°ì˜ˆìš”!\níŒ€ í”„ë¡œì íŠ¸ì™€ ê´€ë ¨ëœ ì§ˆë¬¸ì—ë§Œ ì‘ë‹µí•  ìˆ˜ ìˆì–´ìš”.\n\nì˜ˆ:\nâ€¢ "ì´ íŒ€ì˜ í”„ë¡œì íŠ¸ëŠ” ì–´ë–¤ í”„ë¡œì íŠ¸ì•¼?"\nâ€¢ "ì–´ë–¤ ê¸°ëŠ¥ë“¤ì´ ìˆì–´?"\nâ€¢ "ìµœê·¼ì—ëŠ” ì–´ë–¤ ê¸°ëŠ¥ ì¶”ê°€í–ˆì–´?"\n\nì´ëŸ° ì‹ìœ¼ë¡œ ì§ˆë¬¸í•´ ì£¼ì‹œë©´ ì—´ì‹¬íˆ ë„ì™€ë“œë¦´ê²Œìš”! â˜ºï¸"""

load_dotenv()

QDRANT_URL = os.getenv("QDRANT_URL")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")
QDRANT_COLLECTION = os.getenv("QDRANT_COLLECTION", "github_docs")

# ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ë¥¼ ìœ„í•œ LLM ë˜í¼ (SSE ìš©ë„)
class StreamingLLMWrapper(Runnable):
    def __init__(self, llm):
        self.llm = llm

    def invoke(self, input, config=None):
        raise NotImplementedError("Only streaming is supported in this wrapper.")

    async def astream(self, input, config=None):
        async for token_chunk_from_llm in self.llm.astream(input, config=config):
            
            for char in token_chunk_from_llm:
                if char == '\n':
                    yield '\\n'
                else:
                    yield char

class WeightedQdrantRetriever(BaseRetriever):
    vectorstore: QdrantVectorStore
    top_k: int = 5
    project_id: int

    def _get_relevant_documents(self, query: str, *, config=None) -> List[Document]:
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        query_embedding = self.vectorstore.embeddings.embed_query(query)
        
        results = self.vectorstore.client.search(
            collection_name=self.vectorstore.collection_name,
            query_vector=query_embedding,
            limit=self.top_k,
            with_payload=True,
            query_filter=Filter(
                must=[FieldCondition(key="project_id", match=MatchValue(value=self.project_id))]
            )
        )
        
        docs = []
        for item in results:
            payload = item.payload or {}
            distance = item.score
            cosine_score = 1 - distance
            weight = payload.get("weight", 1.0)
            adjusted_score = cosine_score * weight
            metadata = {
                **payload,
                "raw_distance": distance,
                "cosine_score": cosine_score,
                "adjusted_score": adjusted_score
            }
            docs.append(Document(page_content=payload.get("document", ""), metadata=metadata))
        # docs = sorted(docs, key=lambda d: d.metadata.get("adjusted_score", 0.0), reverse=True)
        docs = sorted(docs, key=lambda d: d.metadata.get("cosine_score", 0.0), reverse=True)
        return docs

# ì„ë² ë”© ëª¨ë¸ ë° ë²¡í„°ìŠ¤í† ì–´ ë¡œë”©
embedding_model = HuggingFaceEmbeddings(
    model_name="BAAI/bge-m3",
    encode_kwargs={"normalize_embeddings": True}
)

# Qdrant client ë° LangChain vectorstore ë˜í¼
qdrant_client = QdrantClient(
    url=QDRANT_URL,
    api_key=QDRANT_API_KEY
)

vectorstore = QdrantVectorStore(
    client=qdrant_client,
    collection_name=QDRANT_COLLECTION,
    embedding=embedding_model,
    content_payload_key="document",
)

# LLM (HyperCLOVA)
# llm = HyperClovaLangChainLLM()
llm = GeminiLangChainLLM()

document_prompt = PromptTemplate(
    input_variables=["page_content"],
    template="- {page_content}"
)

# ì‹¤í–‰ í•¨ìˆ˜
@traceable
def run_rag(question: str, project_id: int) -> str:
    # ë²¡í„° DBì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    retriever = vectorstore.as_retriever(
        search_kwargs={
            "k": 40,
            "filter": Filter(
                must=[FieldCondition(key="project_id", match=MatchValue(value=project_id))]
            )
        }
    )

    # í”„ë¡¬í”„íŠ¸ ì„ íƒ: êµ¬ì¡°í™”ëœ ì§ˆë¬¸ì´ë©´ ìœ í˜•ì— ë”°ë¼ ë§ì¶¤ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
    if is_structured_question(question):
        q_type = classify_question_type(question)
        prompt = build_prompt_template(q_type)
    else:
        prompt = general_prompt_template

    # ë¬¸ì„œ + LLM ì¡°í•© ì²´ì¸ ìƒì„±
    combine_docs_chain = create_stuff_documents_chain(
        llm=llm,
        prompt=prompt,
        document_prompt=document_prompt,
    )
    rag_chain = create_retrieval_chain(retriever=retriever, combine_docs_chain=combine_docs_chain)

    # ë¬¸ì„œ ê²€ìƒ‰ ìˆ˜í–‰
    docs = retriever.invoke(question)
    if not docs:
        return FILTERED_RESPONSE

    # ê¸°ì¤€ ì´í•˜ ì ìˆ˜ì´ë©´ ì‘ë‹µ í•„í„°ë§
    top_score = docs[0].metadata.get("adjusted_score", 1.0)
    if top_score < 0.6:
        return FILTERED_RESPONSE

    # ì²´ì¸ ì‹¤í–‰
    result = rag_chain.invoke({"input": question, "question": question, "context": docs})
    answer = result.get("answer", "")
    return answer.split("ë‹µë³€:", 1)[-1].strip() if "ë‹µë³€:" in answer else answer.strip()

@traceable
async def run_rag_streaming(question: str, project_id: int):
    # ë¬¸ì„œ ê²€ìƒ‰ê¸° êµ¬ì„± (project_id í•„í„° í¬í•¨)
    retriever = WeightedQdrantRetriever(
        vectorstore=vectorstore,
        project_id=project_id,
        top_k=40
    )

    # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    docs = retriever.invoke(question)
    
    # LangSmithì— ë¬¸ì„œ ì •ë³´ traceableí•˜ê²Œ ë‚¨ê¸°ê¸°
    retrieved_doc_metadata = [
    {
        **doc.metadata, # ëª¨ë“  ë©”íƒ€ë°ì´í„° í¬í•¨
        "page_content": doc.page_content[:300] # ì„ íƒì ìœ¼ë¡œ ì¼ë¶€ ë‚´ìš© í¬í•¨
    }
    for doc in docs
]

    config = RunnableConfig(tags=["run_rag_streaming"])

    if config and config.get("callbacks"):
        run_id = config.get("callbacks")[0].current_run_id
        if run_id:
            ls_client = client.Client()
            retrieved_doc_metadata = [
                {
                    **doc.metadata,
                    "page_content": doc.page_content[:300]
                }
                for doc in docs
            ]
            ls_client.update_run(
                run_id,
                inputs={
                    "question": question,
                    "retrieved_docs": retrieved_doc_metadata
                }
            )
        
    adjusted_scores = [doc.metadata.get("adjusted_score", 0.0) for doc in docs[:5]]
    avg_adjusted_scores = sum(adjusted_scores) / max(len(adjusted_scores), 1)
    print(f"â— avg_adjusted_scores: {avg_adjusted_scores}")
    if not docs or avg_adjusted_scores < 0.3:
        # FILTERED_RESPONSEì˜ ê° ê¸€ìë¥¼ ìˆœíšŒí•˜ë©° yieldí•˜ë˜, ì¤„ë°”ê¿ˆì€ ì¹˜í™˜
        for char in FILTERED_RESPONSE:
            if char == '\n':
                yield '\\n' # ì¤„ë°”ê¿ˆ ë¬¸ìë¥¼ íŠ¹ì • ë¬¸ìì—´ë¡œ ì¹˜í™˜í•˜ì—¬ yield
            else:
                yield char
        return

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    if is_structured_question(question):
        q_type = classify_question_type(question)
        prompt_template = build_prompt_template(q_type)
    else:
        prompt_template = general_prompt_template

    # í”„ë¡¬í”„íŠ¸ ì…ë ¥ êµ¬ì„±
    context = "\n".join([doc.page_content for doc in docs])
    prompt_input = {
        "question": question,
        "context": context
    }

    # LangSmith ì¶”ì ìš© config
    config = RunnableConfig(tags=["run_rag_streaming"])

    # ìŠ¤íŠ¸ë¦¬ë° LLM ì²´ì¸ êµ¬ì„±
    chain = (
        RunnableLambda(lambda x: prompt_template.format(**x)) |
        StreamingLLMWrapper(llm)
    )

    # ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
    async for char_chunk in chain.astream(prompt_input, config=config):
        yield char_chunk
import os
import re
import logging
from dotenv import load_dotenv
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch
from huggingface_hub import login
from app.context_construction.query_rewriter import GemmaPrompt
from app.fast_api.schemas.comment_schemas import CommentRequest
from typing import Optional

# ----------------------------
# ë¡œê¹… ì„¤ì •
# ----------------------------

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ----------------------------
# ìƒìˆ˜ ì •ì˜
# ----------------------------

MODEL_NAME = "google/gemma-3-1b-it"
FALLBACK_COMMENT = '{\n"content": "ê°œë°œì ì…ì¥ì—ì„œ ì •ë§ í•„ìš”í•œ ì„œë¹„ìŠ¤ ê°™ì•„ìš”, ëŒ€ë‹¨í•©ë‹ˆë‹¤! ğŸ™Œ" \n}'
CPU_DEVICE = torch.device("cpu")
MAX_NEW_TOKENS = 200
TEMPERATURE = 0.9
TOP_P = 0.9

# ----------------------------
# GemmaModel í´ë˜ìŠ¤
# ----------------------------

class GemmaModel:
    """
    Gemma ëª¨ë¸ì„ ì‚¬ìš©í•´ ëŒ“ê¸€ ìƒì„±ì„ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤
    """
    _is_authenticated = False
    def __init__(self):
        self._authenticate_huggingface()
        self.model_name = MODEL_NAME
        self.device = CPU_DEVICE
        self.tokenizer = None
        self.model = None
        self.pipe = None
        logger.info("Device ì„¤ì •: DeviceëŠ” ì˜ë„ì ìœ¼ë¡œ CPUë¡œ ê³ ì •ë©ë‹ˆë‹¤.")
    
    def _authenticate_huggingface(self) -> None:
        """
        .env íŒŒì¼ì—ì„œ HF_AUTH_TOKENì„ ë¡œë“œí•˜ê³  ë¡œê·¸ì¸í•œë‹¤.
        ì¸ì¦ ìµœì í™”ë¥¼ ìœ„í•´ _is_authenticatedê°€ Falseì¼ ê²½ìš°ë§Œ ë¡œê·¸ì¸í•œë‹¤.
        """
        if GemmaModel._is_authenticated:
            logger.info("Hugging Face ì¸ì¦ ì´ë¯¸ ì™„ë£Œë¨.")
            return  # ì´ë¯¸ ì¸ì¦ë¨ â†’ ê·¸ëƒ¥ ë„˜ì–´ê°

        load_dotenv()
        token = os.getenv("HF_AUTH_TOKEN")
        if not token:
            raise EnvironmentError("HF_AUTH_TOKEN is not set in .env file.")
        login(token=token)

        GemmaModel._is_authenticated = True  # ì¸ì¦ ì™„ë£Œ ì²˜ë¦¬
        logger.info("Hugging Face ì¸ì¦ ì™„ë£Œ.")

    def load_gemma(self) -> None:
        """
        ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•˜ì—¬ íŒŒì´í”„ë¼ì¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        ì´ë¯¸ ì´ˆê¸°í™”ëœ ê²½ìš° ì•„ë¬´ ì‘ì—…ë„ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        """
        if self.pipe is None: 
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForCausalLM.from_pretrained(self.model_name).to(self.device)
            self.pipe = pipeline(
                "text-generation", 
                model=self.model, 
                tokenizer=self.tokenizer, 
                device=-1, 
                temperature=TEMPERATURE, 
                top_p=TOP_P, 
                do_sample=True
            )
            logger.info("Gemma ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")

    # ëŒ“ê¸€ ìƒì„±
    def generate_comment(self, request_data: CommentRequest) -> str:
        """
        í”„ë¡¬í”„íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëŒ“ê¸€ì„ ìƒì„±í•˜ê³  JSON í¬ë§·ìœ¼ë¡œ í›„ì²˜ë¦¬í•©ë‹ˆë‹¤.
        ìƒì„±ëœ ê²°ê³¼ê°€ ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©´ fallback ë©”ì‹œì§€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        prompt_builder = GemmaPrompt(request_data)
        prompt: str = prompt_builder.generate_prompt()

        logger.info("ëŒ“ê¸€ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        outputs = self.pipe(prompt, max_new_tokens = MAX_NEW_TOKENS)[0]["generated_text"]
        output_text = outputs[len(prompt):].strip()

        #JSON ë¸”ë¡ ì¶”ì¶œ
        try:
            logger.info("llm ëŒ“ê¸€ ì¶”ì¶œ ì„±ê³µ : %s", output_text)
            find_comment = re.findall(r'{.*?}', output_text, re.DOTALL)
            generated_comment = find_comment[0].strip()
            logger.info("JSON í˜•ì‹ì˜ ëŒ“ê¸€ ì¶”ì¶œ ì„±ê³µ.")
        except IndexError:
            logger.warning("JSON ì¶”ì¶œ ì‹¤íŒ¨ â†’ fallback ë©”ì‹œì§€ ì‚¬ìš©.")
            generated_comment = FALLBACK_COMMENT
    
        return generated_comment
    

# ì‹±ê¸€í„´ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì„œë²„ ì‹œì‘ ì‹œ 1íšŒë§Œ ì‹¤í–‰)
gemma_model_instance = GemmaModel()
gemma_model_instance.load_gemma()

# ----------------------------
# í…ŒìŠ¤íŠ¸ ì½”ë“œ
# ---------------------------- 

if __name__ == "__main__":
    import time

    dummy_data = CommentRequest(
        comment_type="ì¹­ì°¬",
        team_projectName="í’ˆì•—ì´ ì„œë¹„ìŠ¤",
        team_shortIntro="ì„œë¡œì˜ í”„ë¡œì íŠ¸ ì›¹í˜ì´ì§€ë¥¼ ë°©ë¬¸í•˜ì—¬ íŠ¸ë˜í”½ì„ ëŠ˜ë ¤ì¤Œ",
        team_deployedUrl="https://resume.site",
        team_githubUrl="https://github.com/example",
        team_description="FastAPI + React ê¸°ë°˜ í”„ë¡œì íŠ¸",
        team_tags=["AI", "Gemma", "GCP", "UI ì¹œê·¼í•¨"],
    )
    

    start = time.time()
    logger.info("í…ŒìŠ¤íŠ¸ ì‹œì‘.")
    gemma = GemmaModel(dummy_data)
    gemma.load_gemma()
    comment = gemma.model_inference()
    logger.info("ìƒì„±ëœ ëŒ“ê¸€: %s", comment)
    logger.info("ì‹¤í–‰ ì‹œê°„: %.2fì´ˆ", time.time() - start)
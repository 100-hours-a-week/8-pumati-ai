from dotenv import load_dotenv
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch
from huggingface_hub import login
from faker import Faker
import os
import re


from app.context_construction.query_rewriter import GemmaPrompt
from app.fast_api.schemas.comment_schemas import CommentRequest

import time


# JSON ë°ì´í„° ëª¨ë¸ ì •ì˜
class GemmaModel:
    def __init__(self, data: CommentRequest):
        load_dotenv()
        login(token=os.getenv("HF_AUTH_TOKEN"))

        self.model_name = "google/gemma-3-1b-it"
        self.device = torch.device("cpu")
        
        self.tokenizer = None
        self.model = None
        self.pipe = None

        self.data = data

    def load_gemma(self): #ì²˜ìŒì— ì„œë²„ë¥¼ ë„ìš¸ ë•Œ(getìš”ì²­)ìœ¼ë¡œ ëª¨ë¸ ì‹¤í–‰ì‹œí‚¬ ì˜ˆì •.
        if self.pipe is None:  # ì´ë¯¸ ë¡œë“œí•œ ê²½ìš° ë‹¤ì‹œ ë¡œë”© ì•ˆ í•˜ê²Œ -> ì„œë²„ ì•ˆì •ì„±ì„ ë†’ì´ê¸° ìœ„í•¨.
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForCausalLM.from_pretrained(self.model_name).to(self.device)
            self.pipe = pipeline("text-generation", model=self.model, tokenizer=self.tokenizer, device=-1, temperature=0.9, top_p=0.9, do_sample=True)



    # ì¼ê¸° ìƒì„±
    def model_inference(self):
        prompt_builder = GemmaPrompt(self.data)
        prompt = prompt_builder.generate_prompt()
        output = self.pipe(prompt, max_new_tokens=200)[0]["generated_text"]
        comment_string = output[len(prompt):].strip()
        print(comment_string)

        #ìƒì„±ëœ ëŒ“ê¸€ ì¤‘ JSON ì•ˆì— ìˆëŠ” ëŒ“ê¸€ë§Œ ê°€ì ¸ì˜¤ê¸°.
        try:
            find_comment = re.findall(r'{.*?}', comment_string, re.DOTALL)
            generated_comment = find_comment[0].strip()
        except:
            
            generated_comment = '{\n"content": "ê°œë°œì ì…ì¥ì—ì„œ ì •ë§ í•„ìš”í•œ ì„œë¹„ìŠ¤ ê°™ì•„ìš”, ëŒ€ë‹¨í•©ë‹ˆë‹¤! ğŸ™Œ" \n}'
    
        return generated_comment
    

if __name__ == "__main__":

    dummy_data = CommentRequest(
        comment_type="ì¹­ì°¬",
        team_projectName="AI ì´ë ¥ì„œ ìƒì„±ê¸°",
        team_shortIntro="LLM ê¸°ë°˜ ì´ë ¥ì„œ ìë™ ìƒì„±",
        team_deployedUrl="https://resume.site",
        team_githubUrl="https://github.com/example",
        team_description="FastAPI + React ê¸°ë°˜ í”„ë¡œì íŠ¸",
        team_tags=["AI", "LLM", "FastAPI"])

    
    start = time.time()
    print("start")
    gemma = GemmaModel(dummy_data)
    gemma.load_gemma()
    comment = gemma.model_inference()
    print(f"ìƒì„±ëœ ëŒ“ê¸€: {comment}")
    print(time.time() - start)
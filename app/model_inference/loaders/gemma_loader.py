import os
import re
import json
import logging
from dotenv import load_dotenv
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch
from huggingface_hub import login
from app.context_construction.query_rewriter import GemmaPrompt
from app.fast_api.schemas.comment_schemas import CommentRequest
from typing import Optional

# ----------------------------
# ë¡œê¹… ì„¤ì •
# ----------------------------

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ----------------------------
# ìƒìˆ˜ ì •ì˜
# ----------------------------

MODEL_NAME = "google/gemma-3-1b-it"
FALLBACK_COMMENT = '{\n"content": "ê°œë°œì ì…ì¥ì—ì„œ ì •ë§ í•„ìš”í•œ ì„œë¹„ìŠ¤ ê°™ì•„ìš”, ëŒ€ë‹¨í•©ë‹ˆë‹¤! ğŸ™Œ" \n}'
CPU_DEVICE = torch.device("cpu")
MAX_NEW_TOKENS = 200
TEMPERATURE = 0.9
TOP_P = 0.95
REPETITION_PENALTY = 1.2
MAX_RETRY = 10


# ----------------------------
# GemmaModel í´ë˜ìŠ¤
# ----------------------------

class GemmaModel:
    """
    Gemma ëª¨ë¸ì„ ì‚¬ìš©í•´ ëŒ“ê¸€ ìƒì„±ì„ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤
    """
    _is_authenticated = False
    def __init__(self):
        self._authenticate_huggingface()
        self.model_name = MODEL_NAME
        self.device = CPU_DEVICE
        self.tokenizer = None
        self.model = None
        self.pipe = None
        logger.info("Device ì„¤ì •: DeviceëŠ” ì˜ë„ì ìœ¼ë¡œ CPUë¡œ ê³ ì •ë©ë‹ˆë‹¤.")
    
    def _authenticate_huggingface(self) -> None:
        """
        .env íŒŒì¼ì—ì„œ HF_AUTH_TOKENì„ ë¡œë“œí•˜ê³  ë¡œê·¸ì¸í•œë‹¤.
        ì¸ì¦ ìµœì í™”ë¥¼ ìœ„í•´ _is_authenticatedê°€ Falseì¼ ê²½ìš°ë§Œ ë¡œê·¸ì¸í•œë‹¤.
        """
        if GemmaModel._is_authenticated:
            logger.info("Hugging Face ì¸ì¦ ì´ë¯¸ ì™„ë£Œë¨.")
            return  # ì´ë¯¸ ì¸ì¦ë¨ â†’ ê·¸ëƒ¥ ë„˜ì–´ê°

        load_dotenv()
        token = os.getenv("HF_AUTH_TOKEN")
        if not token:
            raise EnvironmentError("HF_AUTH_TOKEN is not set in .env file.")
        login(token=token)

        GemmaModel._is_authenticated = True  # ì¸ì¦ ì™„ë£Œ ì²˜ë¦¬
        logger.info("Hugging Face ì¸ì¦ ì™„ë£Œ.")

    def load_gemma(self) -> None:
        """
        ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•˜ì—¬ íŒŒì´í”„ë¼ì¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        ì´ë¯¸ ì´ˆê¸°í™”ëœ ê²½ìš° ì•„ë¬´ ì‘ì—…ë„ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        """
        if self.pipe is None: 
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForCausalLM.from_pretrained(self.model_name).to(self.device)
            self.pipe = pipeline(
                "text-generation", 
                model=self.model, 
                tokenizer=self.tokenizer, 
                device=-1, 
                temperature=TEMPERATURE, 
                top_p=TOP_P, 
                do_sample=True,
                repetition_penalty=REPETITION_PENALTY
            )
            logger.info("Gemma ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")

    # ëŒ“ê¸€ ìƒì„±
    def validate_generated_comment(self, generated_comment_dict: dict) -> bool:
        """
        ìƒì„±ëœ ëŒ“ê¸€ JSONì´ ìœ íš¨í•œì§€ ê²€ì‚¬.
        í•„ìˆ˜ í•„ë“œ(content)ê°€ ì—†ê±°ë‚˜ ê°’ì´ ë¹„ì–´ìˆìœ¼ë©´ False
        """
        if "content" not in generated_comment_dict:
            return False

        content = generated_comment_dict["content"]

        if not isinstance(content, str) or not content.strip():
            return False
        
        if len(content.split()) <= 2:
            return False

        return True


    def generate_comment(self, request_data: CommentRequest) -> dict:
        prompt_builder = GemmaPrompt(request_data)
        prompt: str = prompt_builder.generate_prompt()

        for attempt in range(1, MAX_RETRY + 1):
            logger.info(f"ëŒ“ê¸€ ìƒì„± ì‹œë„ {attempt}íšŒ")
            outputs = self.pipe(prompt, max_new_tokens=MAX_NEW_TOKENS)[0]["generated_text"]
            output_text = outputs[len(prompt):].strip()

            try:
                find_comment = re.findall(r'{.*?}', output_text, re.DOTALL)
                generated_comment_str = find_comment[0].strip()
                generated_comment_dict = json.loads(generated_comment_str)

                # âœ… ìœ íš¨ì„± ê²€ì‚¬ ì¶”ê°€ (ì´ê²Œ í•µì‹¬)
                if self.validate_generated_comment(generated_comment_dict):
                    logger.info("JSON íŒŒì‹± + ìœ íš¨ì„± ê²€ì‚¬ ì„±ê³µ.")
                    generated_comment = generated_comment_dict.get("content", "").strip()
                    return generated_comment
                else:
                    raise ValueError("ìƒì„±ëœ JSONì— contentê°€ ì—†ê±°ë‚˜ ë¹„ì–´ ìˆìŒ")

            except (IndexError, json.JSONDecodeError, ValueError) as e:
                logger.warning(f"ëŒ“ê¸€ ìƒì„± ì‹¤íŒ¨ (ì‹œë„ {attempt}íšŒ): {e}")

                if attempt < MAX_RETRY:
                    continue
                else:
                    logger.warning("ìµœëŒ€ ì¬ì‹œë„ ë„ë‹¬ â†’ fallback ì‚¬ìš©.")
                    return json.loads(FALLBACK_COMMENT)


    

# ì‹±ê¸€í„´ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì„œë²„ ì‹œì‘ ì‹œ 1íšŒë§Œ ì‹¤í–‰)
gemma_model_instance = GemmaModel()
gemma_model_instance.load_gemma()

# ----------------------------
# í…ŒìŠ¤íŠ¸ ì½”ë“œ
# ---------------------------- 

if __name__ == "__main__":
    import time

    dummy_data = CommentRequest(
        comment_type="ì¹­ì°¬",
        team_projectName="í’ˆì•—ì´ ì„œë¹„ìŠ¤",
        team_shortIntro="ì„œë¡œì˜ í”„ë¡œì íŠ¸ ì›¹í˜ì´ì§€ë¥¼ ë°©ë¬¸í•˜ì—¬ íŠ¸ë˜í”½ì„ ëŠ˜ë ¤ì¤Œ",
        team_deployedUrl="https://resume.site",
        team_githubUrl="https://github.com/example",
        team_description="FastAPI + React ê¸°ë°˜ í”„ë¡œì íŠ¸",
        team_tags=["AI", "Gemma", "GCP", "UI ì¹œê·¼í•¨"],
    )
    

    start = time.time()
    logger.info("í…ŒìŠ¤íŠ¸ ì‹œì‘.")
    gemma = GemmaModel(dummy_data)
    gemma.load_gemma()
    comment = gemma.model_inference()
    logger.info("ìƒì„±ëœ ëŒ“ê¸€: %s", comment)
    logger.info("ì‹¤í–‰ ì‹œê°„: %.2fì´ˆ", time.time() - start)
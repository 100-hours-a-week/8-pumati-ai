from langchain_core.language_models import LLM
from typing import List, Optional
import google.generativeai as genai
import os
from pydantic import PrivateAttr
from typing import List, Optional
from langchain_core.callbacks import CallbackManagerForLLMRun
import re


# Gemini API í‚¤ ì„¤ì •
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

class GeminiLangChainLLM(LLM):
    model_name: str = "gemini-1.5-flash"
    _model: genai.GenerativeModel = PrivateAttr()

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self._model = genai.GenerativeModel(self.model_name)

    async def astream(self, prompt: str, **kwargs):
        chat = self._model.start_chat()
        stream = chat.send_message(prompt, stream=True)

        for chunk in stream:
            new_text = chunk.text or ""
            yield new_text # LLMì´ ë°˜í™˜í•˜ëŠ” ì›ë³¸ ë¬¸ìì—´ ì²­í¬ë¥¼ ê·¸ëŒ€ë¡œ ì „ë‹¬

        # ì „ì²´ ê²°ê³¼ ëˆ„ì í•´ì„œ í•œ ë²ˆì— ë³´ê¸° (ë””ë²„ê¹…ìš©)
        final_full_response = chat.history[-1].parts[0].text if chat.history else ""
        print("ğŸ§¾ Gemini full response (for debug):", repr(final_full_response.replace('\n', '\\n')))
            
    @property
    def streaming(self) -> bool:
        return True

    @property
    def _llm_type(self) -> str:
        return "gemini"

    def _call(self, prompt: str, stop: Optional[List[str]] = None,
              run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs) -> str:
        chat = self._model.start_chat()
        response = chat.send_message(prompt)
        return response.text
#HyperCLOVA_loader.py

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import os
from huggingface_hub import login
from dotenv import load_dotenv
import re

# .env ë¡œë“œ
load_dotenv()

hf_token = os.getenv("HF_AUTH_TOKEN")
if hf_token:
    login(token=hf_token)
else:
    raise ValueError("HF_AUTH_TOKEN is not set in your .env file!")

model_id = "naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-1.5B"

tokenizer = AutoTokenizer.from_pretrained(
    model_id,
    use_fast=True,             # âš ï¸ Fast tokenizer ê°•ì œ ì‚¬ìš©
    trust_remote_code=True,   # ì»¤ìŠ¤í…€ íŒŒì´ì¬ ì½”ë“œëŠ” ì—†ìœ¼ë¯€ë¡œ êº¼ë‘ 
    use_auth_token=hf_token,   # HF ì¸ì¦ í† í°
)


# device = torch.device("mps") if torch.backends.mps.is_available() else torch.device("cpu")
device = torch.device("cpu")
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    trust_remote_code=True,
    use_auth_token=hf_token,
    torch_dtype=torch.float32,
).to(device)
# print(f"ğŸ—„ï¸ Model is on: {next(model.parameters()).device}")  # â† ì—¬ê¸° ì¶”ê°€

def generate_fortune_text(prompt: str) -> str:
    # inputs = tokenizer(prompt, return_tensors="pt").to(device)
    # print(f"ğŸ“¥ Inputs are on: {inputs.input_ids.device}")
    # with torch.no_grad():
    #     outputs = model.generate(
    #         **inputs,
    #         max_new_tokens=300,
    #         do_sample=True,
    #         temperature=0.7,
    #         top_p=0.9
    #     )
    # return tokenizer.decode(outputs[0], skip_special_tokens=True)

    # 1) í† í°í™” & ë””ë°”ì´ìŠ¤ ì´ë™
    inputs = tokenizer(prompt, return_tensors="pt").to(device)

    # 2) ìƒì„±
    with torch.no_grad():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=300,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
        )[0]
    # 3) í”„ë¡¬í”„íŠ¸ ê¸¸ì´ë§Œí¼ ìŠ¬ë¼ì´ìŠ¤í•´ì„œ 'ìƒì„±ëœ ë¶€ë¶„ë§Œ' ì·¨í•œë‹¤
    gen_ids = output_ids[ inputs["input_ids"].shape[-1] : ]

    # 4) JSON block ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§)
    raw = tokenizer.decode(gen_ids, skip_special_tokens=True)
    match = re.search(r"\{[\s\S]*?\}", raw)
    return match.group() if match else raw
# import os
# import logging
# from dotenv import load_dotenv
# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
# from sentence_transformers import SentenceTransformer
# import torch
# from huggingface_hub import login
# from fast_api.schemas.comment_schemas import CommentRequest
# from context_construction.prompt.comment_prompt import GemmaPrompt

# # ----------------------------
# # ë¡œê¹… ì„¤ì •
# # ----------------------------

# logging.basicConfig(level=logging.INFO)
# logger = logging.getLogger(__name__)

# # ----------------------------
# # ìƒìˆ˜ ì •ì˜
# # ----------------------------

# #gemmaë¡œ ëª¨ë¸ ë³€ê²½
# MODEL_NAME ="google/gemma-3-1b-it" #"naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-1.5B"
# FALLBACK_COMMENT = "ê°œë°œì ì…ì¥ì—ì„œ ì •ë§ í•„ìš”í•œ ì„œë¹„ìŠ¤ ê°™ì•„ìš”, ëŒ€ë‹¨í•©ë‹ˆë‹¤! ğŸ™Œ"
# EMBEDDING_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"#"intfloat/multilingual-e5-small"
# CPU_DEVICE = torch.device("cpu")
# MAX_NEW_TOKENS = 80
# MAX_NEW_TOKENS_SUMMARY = 50
# TEMPERATURE = 0.9
# TOP_P = 0.9
# REPETITION_PENALTY = 1.2
# MAX_RETRY = 60


# # ----------------------------
# # GemmaModel í´ë˜ìŠ¤
# # ----------------------------

# class GemmaModel:
#     """
#     Gemma ëª¨ë¸ì„ ì‚¬ìš©í•´ ëŒ“ê¸€ ìƒì„±ì„ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤
#     """
#     _is_authenticated = False
#     def __init__(self):
#         self._authenticate_huggingface()
#         self.model_name = MODEL_NAME
#         self.device = CPU_DEVICE
#         self.tokenizer = None
#         self.model = None
#         self.pipe = None
#         logger.info(f"Device ì„¤ì •: DeviceëŠ” ì˜ë„ì ìœ¼ë¡œ CPUë¡œ ê³ ì •ë©ë‹ˆë‹¤.")
#         self.embed_model = None
        
    
#     def _authenticate_huggingface(self) -> None:
#         """
#         .env íŒŒì¼ì—ì„œ HF_AUTH_TOKENì„ ë¡œë“œí•˜ê³  ë¡œê·¸ì¸í•œë‹¤.
#         ì¸ì¦ ìµœì í™”ë¥¼ ìœ„í•´ _is_authenticatedê°€ Falseì¼ ê²½ìš°ë§Œ ë¡œê·¸ì¸í•œë‹¤.
#         """
#         if GemmaModel._is_authenticated:
#             logger.info("Hugging Face ì¸ì¦ ì´ë¯¸ ì™„ë£Œë¨.")
#             return  # ì´ë¯¸ ì¸ì¦ë¨ â†’ ê·¸ëƒ¥ ë„˜ì–´ê°

#         load_dotenv()
#         token = os.getenv("HF_AUTH_TOKEN")
#         if not token:
#             raise EnvironmentError("HF_AUTH_TOKEN is not set in .env file.")
#         login(token=token)

#         GemmaModel._is_authenticated = True  # ì¸ì¦ ì™„ë£Œ ì²˜ë¦¬
#         logger.info("Hugging Face ì¸ì¦ ì™„ë£Œ.")


#     def load_gemma(self) -> None:
#         """
#         ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•˜ì—¬ íŒŒì´í”„ë¼ì¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
#         ì´ë¯¸ ì´ˆê¸°í™”ëœ ê²½ìš° ì•„ë¬´ ì‘ì—…ë„ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
#         """
#         if self.pipe is None: 
#             self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
#             self.model = AutoModelForCausalLM.from_pretrained(self.model_name).to(self.device)
#             self.pipe = pipeline(
#                 "text-generation", 
#                 model=self.model, 
#                 tokenizer=self.tokenizer, 
#                 device=-1, 
#                 temperature=TEMPERATURE, 
#                 top_p=TOP_P, 
#                 do_sample=True,
#                 repetition_penalty=REPETITION_PENALTY
#             )
#             self.embed_model = SentenceTransformer(EMBEDDING_MODEL_NAME, device = self.device)
#             logger.info("Gemma ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")
#             logger.info("ì„ë² ë”© ê¸°ë°˜ ê²€ì—´ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")

   
# # ì‹±ê¸€í„´ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì„œë²„ ì‹œì‘ ì‹œ 1íšŒë§Œ ì‹¤í–‰)
# # ìƒì„± ëª©ì : ëŒ“ê¸€ê¸°ëŠ¥ì€ 4ë²ˆ ëª¨ë¸ì„ í˜¸ì¶œí•˜ë©°, í˜¸ì¶œì‹œë§ˆë‹¤ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•  í•„ìš” ì—†ìŒ. ë¶ˆí•„ìš”í•œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±ì„ ì¤„ì´ê³ ì í•¨.
# comment_creator = GemmaModel()
# comment_creator.load_gemma()

import os, logging
from dotenv import load_dotenv
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from sentence_transformers import SentenceTransformer
from huggingface_hub import login
import torch

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

MODEL_NAME = "google/gemma-3-1b-it"
EMBEDDING_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
TEMPERATURE = 0.9
TOP_P = 0.9
REPETITION_PENALTY = 1.2

class GemmaModel:
    _is_authenticated = False

    def __init__(self):
        self._authenticate_huggingface()
        self.model_name = MODEL_NAME
        self.device = torch.device("cpu")
        self.pipe = None
        self.embed_model = None
        self._load_model()

    def _authenticate_huggingface(self):
        if GemmaModel._is_authenticated:
            return
        load_dotenv()
        token = os.getenv("HF_AUTH_TOKEN")
        if not token:
            raise EnvironmentError("HF_AUTH_TOKEN is not set in .env file.")
        login(token=token)
        GemmaModel._is_authenticated = True

    def _load_model(self):
        tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        model = AutoModelForCausalLM.from_pretrained(self.model_name).to(self.device)
        self.pipe = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            device=-1,
            temperature=TEMPERATURE,
            top_p=TOP_P,
            do_sample=True,
            repetition_penalty=REPETITION_PENALTY
        )
        self.embed_model = SentenceTransformer(EMBEDDING_MODEL_NAME, device=self.device)
        logger.info("Gemma ëª¨ë¸ ë° ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

comment_creator = GemmaModel()


# ğŸ“ auth/huggingface_auth.py (í•„ìš” ì‹œ ë”°ë¡œ ë¶„ë¦¬ ê°€ëŠ¥)
# í˜„ì¬ëŠ” comment_loader ë‚´ë¶€ì— ì¸ì¦ í¬í•¨ë¼ ìˆìœ¼ë¯€ë¡œ ìƒëµ ê°€ëŠ¥
# í•˜ì§€ë§Œ ì¶”í›„ auth ëª¨ë“ˆë¡œ ë‚˜ëˆ„ê³  ì‹¶ë‹¤ë©´ ì´ ìœ„ì¹˜ë¡œ í•¨ìˆ˜ ë¶„ë¦¬í•˜ì„¸ìš”

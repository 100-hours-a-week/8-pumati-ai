# ──────────────────────────────────────────────────
# 1. 베이스 이미지 (slim으로 변경)
#    GCP Deep Learning VM PyTorch2.1+CUDA12.1+Ubuntu22.04+Py3.10
# ──────────────────────────────────────────────────
FROM python:3.10-slim

# ──────────────────────────────────────────────────
# 2. 환경 변수
#    - PYTHONPATH에 /workspace/app 추가
#    - 기타 설정
# ──────────────────────────────────────────────────
ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    TOKENIZERS_PARALLELISM=false \
    PYTHONPATH=/workspace/app \
    PIP_NO_CACHE_DIR=1

# ──────────────────────────────────────────────────
# 3. 작업 디렉토리
# ──────────────────────────────────────────────────
WORKDIR /workspace

# ──────────────────────────────────────────────────
# 4. 시스템 패키지 설치 (캐싱 활용)
# ──────────────────────────────────────────────────
RUN --mount=type=cache,target=/var/cache/apt \
    apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    python3-dev \
    git \
    && rm -rf /var/lib/apt/lists/*

# ──────────────────────────────────────────────────
# 5. Python 의존성 설치 (requirements 파일 변경시만 실행)
# ──────────────────────────────────────────────────
COPY requirements.cpu.txt requirements.txt
RUN --mount=type=cache,target=/root/.cache/pip \
    pip install --upgrade pip && \
    pip uninstall -y torch torch_xla || true && \
    pip install -r requirements.txt && \
    pip install --upgrade tokenizers transformers faker

# ──────────────────────────────────────────────────
# 6. 애플리케이션 코드 복사 (자주 변경되는 부분이라 마지막에)
# ──────────────────────────────────────────────────
COPY . /workspace/

# HyperCLOVA 로더 패치 (에러 발생 방지)
RUN sed -i 's/login(token=hf_token)/try:\n    login(token=hf_token)\nexcept Exception as e:\n    print(f"Warning: HyperCLOVA login failed: {e}")/' /workspace/app/model_inference/loaders/HyperCLOVA_loader.py \
    && sed -i 's/from app.model_inference.loaders.HyperCLOVA_loader import generate_fortune_text/try:\n    from app.model_inference.loaders.HyperCLOVA_loader import generate_fortune_text\nexcept Exception:\n    def generate_fortune_text(*args, **kwargs):\n        return "Fortune service is temporarily unavailable"\n/' /workspace/app/model_inference/inference_runner.py

# ──────────────────────────────────────────────────
# 7. 디버그: 빌드 시점에 출력
#    * find, ls, sys.path, fast_api 설치 여부 확인
# ──────────────────────────────────────────────────
RUN echo "===== /workspace/app 구조 =====" \
 && find /workspace/app -maxdepth 2 | sort \
 && echo "\n===== Python sys.path =====" \
 && python3 -c "import sys; print(sys.path)" \
 && echo "\n===== fast_api 패키지 =====" \
 && ls -R /workspace/app/fast_api

# ──────────────────────────────────────────────────
# 8. 포트 노출 및 컨테이너 시작
# ──────────────────────────────────────────────────
EXPOSE 8080

# 9. ENTRYPOINT를 Python -m uvicorn으로 완전 고정
ENTRYPOINT ["python3", "-m", "uvicorn"]
CMD ["app.main_cpu:app", "--host", "0.0.0.0", "--port", "8080"]

# 토큰없다고 에러떠서 빈토큰 추가
#RUN echo "HF_AUTH_TOKEN=dummy_token" > /workspace/.env